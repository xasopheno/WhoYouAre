{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import concatenate, array, asarray\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.core import Activation\n",
    "from keras.callbacks import ReduceLROnPlateau, LambdaCallback\n",
    "from keras.utils import plot_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from NN.ConvolutionalRNN import ConvolutionalRNN\n",
    "from NN.jan_17_Model import SimpleModel\n",
    "from Audio.MidiPlayer import MidiPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of lag hours\n",
    "n_hours = 128\n",
    "n_features = 2\n",
    "n_train_hours = None\n",
    "n_divisions = 4\n",
    "batch_size = n_hours\n",
    "epochs = 150\n",
    "player = MidiPlayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = read_csv('music_data.csv', header=0)\n",
    "values = dataset.values\n",
    "values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify columns to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [0, 1]\n",
    "i = 1\n",
    "# plot each column\n",
    "pyplot.figure()\n",
    "for group in groups:\n",
    "    pyplot.subplot(len(groups), 1, i)\n",
    "    pyplot.plot(values[:, group])\n",
    "    pyplot.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convert series to supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = read_csv('music_data.csv', header=1)\n",
    "n_train_hours = int(len(dataset) * 1)\n",
    "values = dataset.values\n",
    "# values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    start = time.time()\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler.fit(values)\n",
    "# scaled = scaler.transform(values)\n",
    "# print(scaled[20:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values, n_hours, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "train = values[:, :]\n",
    "test = values[:, :]\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(\n",
    "#     categorical_features = [0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[:, :n_obs]\n",
    "test_X = test[:, :n_obs]\n",
    "\n",
    "train_y_volume = test[:, -n_features -1]\n",
    "train_y_notes = test[:, -n_features]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_y_volume = scaler.fit_transform(train_y_volume.reshape(-1,1))\n",
    "\n",
    "\n",
    "print(train_y_volume[0:10])\n",
    "# print(train_y_notes[0:20])\n",
    "\n",
    "test_y_notes = le.fit_transform(train_y_notes)\n",
    "# test_y_notes = ohe.fit_transform(train_y_notes.reshape(-1, 1)).toarray()\n",
    "test_y_notes = np_utils.to_categorical(test_y_notes)\n",
    "# print(test_y_notes[0:10])\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "# print(train_X[0:10])\n",
    "\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "# print(train_X.shape, train_y_notes.shape, test_X.shape, test_y_notes.shape)\n",
    "print(train_y_notes.shape)\n",
    "print(test_y_notes.shape)\n",
    "# test_y_notes = test_y_notes.reshape(n_hours, 45)\n",
    "print(train_X[2])\n",
    "print(test_y_notes[12])\n",
    "print(train_y_volume[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visible = Input(name='input', shape=(n_hours, n_features))\n",
    "conv_rnn = ConvolutionalRNN(visible, n_hours, n_features, n_divisions)\n",
    "simple = SimpleModel(visible, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_rnn.model()\n",
    "# model = simple.model()\n",
    "\n",
    "output_notes = Dense(45, activation='softmax', name='output_notes')(model)\n",
    "output_volume = Dense(1, activation='sigmoid', name='output_volume')(model)\n",
    "\n",
    "model = Model(inputs=[visible], outputs=[\n",
    "                                         output_notes, \n",
    "                                         output_volume, \n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss={'output_notes':'categorical_crossentropy', 'output_volume':'mae'}, optimizer=optimizer)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cache = dataset.values[30:n_hours + 30].tolist()\n",
    "test_data = test_cache.copy()\n",
    "array_to_play = test_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(testx):\n",
    "    testx = DataFrame(data = testx)\n",
    "    testx = testx.values\n",
    "    testx = testx.astype('float32')\n",
    "    testx = scaler.transform(testx)\n",
    "    return(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds[0], 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    global test_cache\n",
    "    global test_data\n",
    "    global array_to_play\n",
    "    if epoch % 10 == 0:\n",
    "        start = time.time()\n",
    "        print('----- Generating sound after: %d' % epoch)\n",
    "        for i in range(400):\n",
    "            if i == 0:\n",
    "                to_add = [100, 80]\n",
    "                array_to_play.append(to_add)\n",
    "            if i == 100 or i == 500:  \n",
    "                rand = random.randint(10, 15)\n",
    "                for i in range(rand):\n",
    "                    to_add = [0, 0]\n",
    "                    array_to_play.append(to_add)\n",
    "                    test_data.append(to_add)\n",
    "                for i in range(rand):\n",
    "                    test_data.pop(0)\n",
    "\n",
    "            if i == 300:    \n",
    "                for i in range(15):\n",
    "                    rand = random.randint(45, 65)\n",
    "                    to_add = [rand, 45]\n",
    "                    array_to_play.append(to_add)\n",
    "                    test_data.append(to_add)\n",
    "                for i in range(15):\n",
    "                    test_data.pop(0)\n",
    "\n",
    "            data = process_data(test_data)\n",
    "            data = data.reshape(1,n_hours,2)\n",
    "            prediction = model.predict(data)\n",
    "\n",
    "#             note_prediction = [np.argmax(y, axis=None, out=None) for y in prediction[0]][0]\n",
    "            note_prediction = sample(prediction[0], temperature = 3)\n",
    "            \n",
    "            note_prediction = int(le.inverse_transform(note_prediction))\n",
    "\n",
    "            volume_prediction = prediction[1][0][0]\n",
    "            volume_prediction = int(scaler.inverse_transform(volume_prediction)[0][0])\n",
    "            prediction = [note_prediction, volume_prediction]\n",
    "            test_data.append(prediction)\n",
    "            test_data.pop(0)\n",
    "            array_to_play.append(prediction)\n",
    "        end = time.time()\n",
    "        print('time:', end - start)\n",
    "        print('len array_to_play', len(array_to_play))\n",
    "        print('array_to_play', array_to_play)\n",
    "        for value in array_to_play:\n",
    "            player.play(value[0], .03, value[1])\n",
    "        test_data = test_cache.copy()\n",
    "        array_to_play = test_cache.copy()\n",
    "\n",
    "    \n",
    "play_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1920/1961 [============================>.] - ETA: 0s - loss: 3.6059 - output_notes_loss: 3.4052 - output_volume_loss: 0.2007----- Generating sound after: 0\n",
      "time: 33.22340703010559\n",
      "len array_to_play 558\n",
      "array_to_play [[76, 55], [76, 66], [57, 48], [57, 0], [57, 51], [81, 57], [57, 40], [57, 0], [57, 40], [80, 53], [80, 0], [80, 64], [80, 48], [69, 47], [69, 0], [81, 58], [80, 43], [80, 21], [80, 21], [84, 40], [84, 61], [84, 61], [84, 60], [83, 55], [53, 38], [53, 0], [69, 27], [53, 2], [50, 13], [50, 0], [57, 15], [57, 14], [57, 14], [57, 7], [57, 4], [69, 7], [69, 0], [69, 5], [57, 2], [57, 0], [57, 0], [57, 0], [72, 2], [72, 0], [72, 7], [72, 4], [72, 3], [72, 3], [64, 3], [76, 15], [64, 0], [64, 0], [64, 0], [77, 11], [77, 0], [77, 17], [76, 8], [76, 0], [76, 0], [67, 3], [67, 0], [67, 0], [67, 0], [65, 0], [65, 14], [65, 14], [65, 13], [65, 8], [65, 8], [76, 7], [64, 10], [64, 0], [76, 0], [0, 0], [74, 1], [74, 0], [74, 7], [74, 9], [53, 11], [53, 0], [53, 18], [72, 21], [72, 0], [72, 16], [72, 9], [72, 9], [0, 6], [0, 0], [64, 1], [64, 0], [0, 0], [68, 10], [68, 6], [68, 6], [68, 3], [80, 5], [80, 18], [80, 18], [80, 9], [80, 6], [80, 5], [80, 5], [64, 5], [64, 6], [64, 6], [64, 3], [55, 2], [74, 10], [74, 0], [74, 9], [74, 4], [72, 6], [72, 0], [72, 14], [72, 11], [84, 7], [84, 0], [71, 4], [52, 11], [52, 0], [71, 0], [71, 0], [69, 0], [69, 0], [69, 6], [68, 3], [69, 0], [69, 0], [100, 80], [78, 54], [82, 54], [100, 80], [72, 51], [57, 51], [51, 51], [64, 51], [50, 51], [85, 51], [74, 51], [62, 51], [53, 51], [73, 51], [59, 51], [60, 51], [64, 51], [71, 51], [70, 51], [74, 51], [69, 51], [70, 51], [82, 51], [77, 51], [77, 51], [61, 51], [84, 51], [67, 51], [57, 51], [81, 51], [88, 51], [71, 51], [78, 51], [55, 51], [86, 51], [84, 51], [63, 51], [72, 51], [86, 51], [65, 51], [73, 51], [55, 51], [67, 51], [74, 51], [63, 51], [44, 51], [80, 51], [76, 51], [44, 51], [76, 51], [82, 51], [0, 51], [60, 51], [88, 51], [86, 51], [68, 51], [54, 51], [75, 51], [50, 51], [69, 51], [80, 51], [69, 51], [76, 51], [57, 51], [77, 51], [55, 51], [68, 51], [68, 51], [66, 51], [86, 51], [68, 51], [75, 51], [56, 51], [85, 51], [80, 51], [69, 51], [49, 51], [78, 51], [74, 51], [73, 51], [58, 51], [69, 51], [77, 51], [83, 51], [64, 51], [50, 51], [66, 51], [57, 51], [74, 51], [45, 51], [86, 51], [74, 51], [78, 51], [59, 51], [83, 51], [58, 51], [74, 51], [70, 51], [83, 51], [78, 51], [62, 51], [54, 51], [70, 51], [70, 51], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 51], [56, 51], [51, 51], [66, 51], [82, 51], [72, 51], [46, 51], [58, 52], [81, 52], [71, 52], [44, 52], [52, 52], [57, 52], [54, 52], [84, 52], [61, 52], [67, 52], [62, 51], [79, 51], [74, 51], [68, 51], [71, 51], [55, 51], [0, 51], [68, 51], [82, 51], [49, 51], [70, 51], [61, 51], [55, 51], [78, 51], [47, 51], [64, 51], [48, 51], [65, 51], [64, 51], [59, 51], [83, 51], [48, 51], [51, 51], [84, 51], [67, 51], [71, 51], [60, 51], [49, 51], [70, 51], [67, 51], [55, 51], [86, 51], [71, 51], [63, 51], [55, 51], [80, 51], [51, 51], [84, 51], [82, 51], [56, 51], [47, 51], [76, 51], [86, 51], [88, 51], [65, 51], [62, 51], [77, 51], [64, 51], [80, 51], [76, 51], [76, 51], [50, 51], [60, 51], [85, 51], [76, 51], [80, 51], [76, 51], [63, 51], [69, 51], [45, 51], [85, 51], [49, 51], [76, 51], [73, 51], [88, 51], [76, 51], [62, 51], [53, 51], [51, 51], [59, 51], [44, 51], [84, 51], [57, 51], [88, 51], [78, 51], [65, 51], [61, 51], [65, 51], [82, 51], [47, 51], [67, 51], [66, 51], [56, 51], [61, 51], [88, 51], [57, 51], [88, 51], [0, 51], [78, 51], [54, 51], [54, 51], [88, 51], [51, 51], [56, 51], [68, 51], [72, 51], [85, 51], [54, 51], [54, 51], [54, 51], [78, 51], [69, 51], [48, 51], [46, 51], [81, 51], [82, 51], [56, 51], [56, 51], [71, 51], [53, 51], [44, 51], [58, 51], [69, 51], [76, 51], [60, 51], [83, 51], [81, 51], [84, 51], [75, 51], [44, 51], [66, 51], [69, 51], [46, 51], [72, 51], [50, 51], [47, 51], [46, 51], [50, 51], [80, 51], [65, 51], [85, 51], [81, 51], [48, 51], [75, 51], [86, 51], [46, 51], [76, 51], [84, 51], [50, 51], [80, 51], [49, 51], [88, 51], [52, 51], [45, 51], [74, 51], [56, 51], [85, 51], [54, 51], [74, 51], [45, 51], [61, 51], [0, 51], [44, 51], [84, 51], [53, 51], [62, 51], [85, 51], [48, 51], [45, 51], [63, 51], [79, 51], [67, 51], [0, 51], [46, 51], [68, 51], [54, 51], [80, 51], [53, 51], [68, 51], [66, 51], [49, 51], [74, 51], [49, 51], [57, 51], [67, 51], [76, 51], [48, 51], [70, 51], [55, 51], [85, 51], [77, 51], [72, 51], [67, 51], [62, 45], [48, 45], [46, 45], [52, 45], [55, 45], [60, 45], [63, 45], [48, 45], [56, 45], [56, 45], [63, 45], [65, 45], [51, 45], [52, 45], [65, 45], [88, 51], [83, 51], [45, 51], [66, 51], [69, 51], [76, 51], [57, 51], [74, 51], [68, 51], [64, 51], [88, 51], [79, 51], [77, 51], [81, 51], [77, 51], [81, 51], [80, 51], [66, 51], [51, 51], [59, 51], [77, 51], [63, 51], [70, 51], [71, 51], [44, 51], [54, 51], [71, 51], [61, 51], [70, 51], [77, 51], [74, 51], [61, 51], [67, 51], [62, 51], [49, 51], [46, 51], [61, 51], [48, 51], [80, 51], [86, 51], [0, 51], [49, 51], [47, 51], [50, 51], [66, 51], [82, 51], [74, 51], [46, 51], [50, 51], [57, 51], [69, 51], [73, 51], [60, 51], [65, 51], [45, 51], [66, 51], [52, 51], [50, 51], [53, 51], [77, 51], [80, 51], [44, 51], [78, 51], [69, 51], [53, 51], [70, 51], [76, 51], [45, 51], [78, 51], [72, 51], [52, 51], [46, 51], [66, 51], [61, 51], [67, 51], [0, 51], [50, 51], [45, 51], [71, 51], [74, 51], [0, 51], [57, 51], [70, 51], [72, 51], [82, 51], [49, 51], [75, 51], [82, 51], [74, 51], [74, 51], [61, 51], [53, 51], [44, 51], [46, 51], [71, 51], [81, 51], [86, 51], [44, 51], [72, 51], [52, 51]]\n",
      "1961/1961 [==============================] - 82s - loss: 3.5915 - output_notes_loss: 3.3930 - output_volume_loss: 0.1985    \n",
      "Epoch 2/150\n",
      " 512/1961 [======>.......................] - ETA: 25s - loss: 3.2804 - output_notes_loss: 3.0993 - output_volume_loss: 0.1811"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "            {'input': train_X},\n",
    "            {\n",
    "                'output_notes': test_y_notes, \n",
    "                'output_volume': train_y_volume, \n",
    "            },\n",
    "            verbose=1,\n",
    "            shuffle=False,\n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[play_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "# pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print('Saved scaler to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}